제목,저자,게시 날짜,저널,권,호,페이지,게시자,설명,전체 인용횟수,학술 문서,컨퍼런스
Dropout: a simple way to prevent neural networks from overfitting,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov",2014/1/1,The journal of machine learning research,15,1,1929-1958,JMLR. org,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",28491회,"Dropout: a simple way to prevent neural networks from overfitting
N Srivastava, G Hinton, A Krizhevsky, I Sutskever… - The journal of machine learning research, 2014
28491회 인용 관련 학술자료 전체 44개의 버전",
Reducing the dimensionality of data with neural networks,"Geoffrey E Hinton, Ruslan R Salakhutdinov",2006/7/28,science,313,5786,504-507,American Association for the Advancement of Science,"High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",15251회,"Reducing the dimensionality of data with neural networks
GE Hinton, RR Salakhutdinov - science, 2006
15251회 인용 관련 학술자료 전체 23개의 버전",
"Show, attend and tell: Neural image caption generation with visual attention","Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, Yoshua Bengio",2015/2/10,,2,3,5,,"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",7141회,"Show, attend and tell: Neural image caption generation with visual attention
K Xu, J Ba, R Kiros, K Cho, A Courville… - International conference on machine learning, 2015
7141회 인용 관련 학술자료 전체 28개의 버전",International Conference on Machine Learning (ICML)
Improving neural networks by preventing co-adaptation of feature detectors,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R Salakhutdinov",2012/7/3,arXiv preprint arXiv:1207.0580,,,,,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This"" overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random"" dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",6456회,"Improving neural networks by preventing co-adaptation of feature detectors
GE Hinton, N Srivastava, A Krizhevsky, I Sutskever… - arXiv preprint arXiv:1207.0580, 2012
6431회 인용 관련 학술자료 전체 26개의 버전
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv: 12070580*
GE Hinton, N Srivastava, A Krizhevsky, I Sutskever… - 2012
39회 인용 관련 학술자료",
Semi-supervised learning using gaussian fields and harmonic functions,"Xiaojin Zhu, Zoubin Ghahramani, John D Lafferty",2003,,,,912-919,,"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.",4246회,"Semi-supervised learning using gaussian fields and harmonic functions
X Zhu, Z Ghahramani, JD Lafferty - Proceedings of the 20th International conference on …, 2003
4246회 인용 관련 학술자료 전체 38개의 버전",Proceedings of the 20th International conference on Machine learning (ICML-03)
Probabilistic matrix factorization,"Andriy Mnih, Ruslan R Salakhutdinov",2008,,,,1257-1264,,"Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a constrained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The resulting model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netflix’s own system.",4124회,"Probabilistic matrix factorization
A Mnih, RR Salakhutdinov - Advances in neural information processing systems, 2007
4109회 인용 관련 학술자료 전체 22개의 버전
Probabilistic matrix factorization*
S Ruslan, M Andriy - Advances in Neural Information Processing Systems, 2008
20회 인용 관련 학술자료",Advances in neural information processing systems
Probabilistic matrix factorization,"Ruslan Salakhutdinov, Andriy Mnih",2007,,21,,,,,4109회,"Probabilistic matrix factorization*
A Mnih, RR Salakhutdinov - Advances in neural information processing systems, 2007
4109회 인용 관련 학술자료 전체 22개의 버전",Neural Information Processing Systems
Deep Boltzmann Machines.,"Ruslan Salakhutdinov, Geoffrey E Hinton",2009/4/2,,1,,8,,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",2755회,"Deep boltzmann machines
R Salakhutdinov, G Hinton - Artificial intelligence and statistics, 2009
2496회 인용 관련 학술자료 전체 27개의 버전
Efficient learning of deep Boltzmann machines*
R Salakhutdinov, H Larochelle - Proceedings of the thirteenth international conference …, 2010
408회 인용 관련 학술자료 전체 19개의 버전",International Conference on Artificial Intelligence and Statistics (AISTATS)
Xlnet: Generalized autoregressive pretraining for language understanding,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le",2019/6/19,arXiv preprint arXiv:1906.08237,,,,,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",2682회,"Xlnet: Generalized autoregressive pretraining for language understanding
Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov… - arXiv preprint arXiv:1906.08237, 2019
2682회 인용 관련 학술자료 전체 14개의 버전",
Skip-thought vectors,"Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler",2015/6/22,arXiv preprint arXiv:1506.06726,,,,,"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",2182회,"Skip-thought vectors
R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 2015
2182회 인용 관련 학술자료 전체 14개의 버전",
