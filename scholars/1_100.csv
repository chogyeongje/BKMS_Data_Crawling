제목,저자,게시 날짜,저널,권,호,페이지,게시자,설명,전체 인용횟수,학술 문서,컨퍼런스,도서,출처,기관
Dropout: a simple way to prevent neural networks from overfitting,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov",2014/1/1,The journal of machine learning research,15,1,1929-1958,JMLR. org,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",28491회,"Dropout: a simple way to prevent neural networks from overfitting
N Srivastava, G Hinton, A Krizhevsky, I Sutskever… - The journal of machine learning research, 2014
28491회 인용 관련 학술자료 전체 44개의 버전",,,,
Reducing the dimensionality of data with neural networks,"Geoffrey E Hinton, Ruslan R Salakhutdinov",2006/7/28,science,313,5786,504-507,American Association for the Advancement of Science,"High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.",15251회,"Reducing the dimensionality of data with neural networks
GE Hinton, RR Salakhutdinov - science, 2006
15251회 인용 관련 학술자료 전체 23개의 버전",,,,
"Show, attend and tell: Neural image caption generation with visual attention","Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S Zemel, Yoshua Bengio",2015/2/10,,2,3,5,,"Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",7141회,"Show, attend and tell: Neural image caption generation with visual attention
K Xu, J Ba, R Kiros, K Cho, A Courville… - International conference on machine learning, 2015
7141회 인용 관련 학술자료 전체 28개의 버전",International Conference on Machine Learning (ICML),,,
Improving neural networks by preventing co-adaptation of feature detectors,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R Salakhutdinov",2012/7/3,arXiv preprint arXiv:1207.0580,,,,,"When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This"" overfitting"" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random"" dropout"" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",6456회,"Improving neural networks by preventing co-adaptation of feature detectors
GE Hinton, N Srivastava, A Krizhevsky, I Sutskever… - arXiv preprint arXiv:1207.0580, 2012
6431회 인용 관련 학술자료 전체 26개의 버전
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv: 12070580*
GE Hinton, N Srivastava, A Krizhevsky, I Sutskever… - 2012
39회 인용 관련 학술자료",,,,
Semi-supervised learning using gaussian fields and harmonic functions,"Xiaojin Zhu, Zoubin Ghahramani, John D Lafferty",2003,,,,912-919,,"An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm’s ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks.",4246회,"Semi-supervised learning using gaussian fields and harmonic functions
X Zhu, Z Ghahramani, JD Lafferty - Proceedings of the 20th International conference on …, 2003
4246회 인용 관련 학술자료 전체 38개의 버전",Proceedings of the 20th International conference on Machine learning (ICML-03),,,
Probabilistic matrix factorization,"Andriy Mnih, Ruslan R Salakhutdinov",2008,,,,1257-1264,,"Many existing approaches to collaborative filtering can neither handle very large datasets nor easily deal with users who have very few ratings. In this paper we present the Probabilistic Matrix Factorization (PMF) model which scales linearly with the number of observations and, more importantly, performs well on the large, sparse, and very imbalanced Netflix dataset. We further extend the PMF model to include an adaptive prior on the model parameters and show how the model capacity can be controlled automatically. Finally, we introduce a constrained version of the PMF model that is based on the assumption that users who have rated similar sets of movies are likely to have similar preferences. The resulting model is able to generalize considerably better for users with very few ratings. When the predictions of multiple PMF models are linearly combined with the predictions of Restricted Boltzmann Machines models, we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netflix’s own system.",4124회,"Probabilistic matrix factorization
A Mnih, RR Salakhutdinov - Advances in neural information processing systems, 2007
4109회 인용 관련 학술자료 전체 22개의 버전
Probabilistic matrix factorization*
S Ruslan, M Andriy - Advances in Neural Information Processing Systems, 2008
20회 인용 관련 학술자료",Advances in neural information processing systems,,,
Probabilistic matrix factorization,"Ruslan Salakhutdinov, Andriy Mnih",2007,,21,,,,,4109회,"Probabilistic matrix factorization*
A Mnih, RR Salakhutdinov - Advances in neural information processing systems, 2007
4109회 인용 관련 학술자료 전체 22개의 버전",Neural Information Processing Systems,,,
Deep Boltzmann Machines.,"Ruslan Salakhutdinov, Geoffrey E Hinton",2009/4/2,,1,,8,,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",2755회,"Deep boltzmann machines
R Salakhutdinov, G Hinton - Artificial intelligence and statistics, 2009
2496회 인용 관련 학술자료 전체 27개의 버전
Efficient learning of deep Boltzmann machines*
R Salakhutdinov, H Larochelle - Proceedings of the thirteenth international conference …, 2010
408회 인용 관련 학술자료 전체 19개의 버전",International Conference on Artificial Intelligence and Statistics (AISTATS),,,
Xlnet: Generalized autoregressive pretraining for language understanding,"Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V Le",2019/6/19,arXiv preprint arXiv:1906.08237,,,,,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",2682회,"Xlnet: Generalized autoregressive pretraining for language understanding
Z Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov… - arXiv preprint arXiv:1906.08237, 2019
2682회 인용 관련 학술자료 전체 14개의 버전",,,,
Skip-thought vectors,"Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, Sanja Fidler",2015/6/22,arXiv preprint arXiv:1506.06726,,,,,"We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.",2182회,"Skip-thought vectors
R Kiros, Y Zhu, R Salakhutdinov, RS Zemel, A Torralba… - arXiv preprint arXiv:1506.06726, 2015
2182회 인용 관련 학술자료 전체 14개의 버전",,,,
Principal manifolds and nonlinear dimensionality reduction via tangent space alignment,"Zhenyue Zhang, Hongyuan Zha",2004,SIAM journal on scientific computing,26,1,313-338,Society for Industrial and Applied Mathematics,"We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements.",2168회,"Principal manifolds and nonlinear dimensionality reduction via tangent space alignment
Z Zhang, H Zha - SIAM journal on scientific computing, 2004
2168회 인용 관련 학술자료 전체 23개의 버전",,,,
Siamese neural networks for one-shot image recognition,"Gregory Koch, Richard Zemel, Ruslan Salakhutdinov",2015/7/10,ICML deep learning workshop,2,,,,"The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class.",2126회,"Siamese neural networks for one-shot image recognition
G Koch, R Zemel, R Salakhutdinov - ICML deep learning workshop, 2015
2126회 인용 관련 학술자료 전체 10개의 버전",,,,
Restricted Boltzmann machines for collaborative filtering,"Ruslan Salakhutdinov, Andriy Mnih, Geoffrey Hinton",2007/6/20,,,,791-798,,"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",2049회,"Restricted Boltzmann machines for collaborative filtering
R Salakhutdinov, A Mnih, G Hinton - Proceedings of the 24th international conference on …, 2007
2049회 인용 관련 학술자료 전체 32개의 버전",,Proceedings of the 24th international conference on Machine learning,,
Unsupervised learning of video representations using lstms,"Nitish Srivastava, Elman Mansimov, Ruslan Salakhutdinov",2015/2/16,,2,,,,"We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences–patches of image pixels and high-level representations (“percepts"") of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem–human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance.",1982회,"Unsupervised learning of video representations using lstms
N Srivastava, E Mansimov, R Salakhudinov - International conference on machine learning, 2015
1982회 인용 관련 학술자료 전체 19개의 버전",International Conference on Machine Learning (ICML),,,
Neighbourhood components analysis,"Jacob Goldberger, Geoffrey E Hinton, Sam Roweis, Russ R Salakhutdinov",2004,Advances in neural information processing systems,17,,513-520,,"In this paper we propose a novel method for learning a Mahalanobis distance measure to be used in the KNN classification algorithm. The algorithm directly maximizes a stochastic variant of the leave-one-out KNN score on the training set. It can also learn a low-dimensional linear embedding of labeled data that can be used for data visualization and fast classification. Unlike other methods, our classification model is non-parametric, making no assumptions about the shape of the class distributions or the boundaries between them. The performance of the method is demonstrated on several data sets, both for metric learning and linear dimensionality reduction.",1970회,"Neighbourhood components analysis
J Goldberger, GE Hinton, S Roweis, RR Salakhutdinov - Advances in neural information processing systems, 2004
1970회 인용 관련 학술자료 전체 20개의 버전",,,,
Human-level concept learning through probabilistic program induction,"Brenden M Lake, Ruslan Salakhutdinov, Joshua B Tenenbaum",2015/12/11,Science,350,6266,1332-1338,American Association for the Advancement of Science,"People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms—for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world’s alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several “visual Turing tests” probing the model’s creative generalization abilities, which in many cases are indistinguishable from human …",1902회,"Human-level concept learning through probabilistic program induction
BM Lake, R Salakhutdinov, JB Tenenbaum - Science, 2015
1902회 인용 관련 학술자료 전체 35개의 버전",,,,
Unifying visual-semantic embeddings with multimodal neural language models,"Ryan Kiros, Ruslan Salakhutdinov, Richard S Zemel",2014/11/10,Transactions of the Association for Computational Linguistics (TACL),,,,,"Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic eg* image of a blue car*-"" blue""+"" red"" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",1522회,"Unifying visual-semantic embeddings with multimodal neural language models
R Kiros, R Salakhutdinov, RS Zemel - arXiv preprint arXiv:1411.2539, 2014
1055회 인용 관련 학술자료 전체 6개의 버전
Multimodal neural language models*
R Kiros, R Salakhutdinov, R Zemel - International conference on machine learning, 2014
640회 인용 관련 학술자료 전체 21개의 버전",,,,
Bayesian probabilistic matrix factorization using Markov chain Monte Carlo,"Ruslan Salakhutdinov, Andriy Mnih",2008/7/5,,,,880-887,ACM,"Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction …",1513회,"Bayesian probabilistic matrix factorization using Markov chain Monte Carlo
R Salakhutdinov, A Mnih - Proceedings of the 25th international conference on …, 2008
1513회 인용 관련 학술자료 전체 27개의 버전",Proceedings of the 25th international conference on Machine learning,,,
Bayesian probabilistic matrix factorization using Markov chain Monte Carlo,"Ruslan Salakhutdinov, Andriy Mnih",2008/7/5,,,,880-887,ACM,"Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction …",1513회,"Bayesian probabilistic matrix factorization using Markov chain Monte Carlo
R Salakhutdinov, A Mnih - Proceedings of the 25th international conference on …, 2008
1513회 인용 관련 학술자료 전체 27개의 버전",Proceedings of the 25th international conference on Machine learning,,,
Bayesian probabilistic matrix factorization using Markov chain Monte Carlo,"Ruslan Salakhutdinov, Andriy Mnih",2008/7/5,,,,880-887,ACM,"Low-rank matrix approximation methods provide one of the simplest and most effective approaches to collaborative filtering. Such models are usually fitted to data by finding a MAP estimate of the model parameters, a procedure that can be performed efficiently even on very large datasets. However, unless the regularization parameters are tuned carefully, this approach is prone to overfitting because it finds a single point estimate of the parameters. In this paper we present a fully Bayesian treatment of the Probabilistic Matrix Factorization (PMF) model in which model capacity is controlled automatically by integrating over all model parameters and hyperparameters. We show that Bayesian PMF models can be efficiently trained using Markov chain Monte Carlo methods by applying them to the Netflix dataset, which consists of over 100 million movie ratings. The resulting models achieve significantly higher prediction …",1513회,"Bayesian probabilistic matrix factorization using Markov chain Monte Carlo
R Salakhutdinov, A Mnih - Proceedings of the 25th international conference on …, 2008
1513회 인용 관련 학술자료 전체 27개의 버전",Proceedings of the 25th international conference on Machine learning,,,
Multimodal Learning with Deep Boltzmann Machines.,"Nitish Srivastava, Ruslan Salakhutdinov",2012/12/3,NIPS,1,,2,,"Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",1314회,"Multimodal Learning with Deep Boltzmann Machines.
N Srivastava, R Salakhutdinov - NIPS, 2012
1314회 인용 관련 학술자료 전체 25개의 버전",,,,
Semantic hashing,"Ruslan Salakhutdinov, Geoffrey Hinton",2009/7/1,International Journal of Approximate Reasoning,50,7,969-978,Elsevier,"We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (e.g. 32), the graphical model performs “semantic hashing”: Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to …",1233회,"Semantic hashing
R Salakhutdinov, G Hinton - International Journal of Approximate Reasoning, 2009
1233회 인용 관련 학술자료 전체 16개의 버전",,,,
Transformer-xl: Attentive language models beyond a fixed-length context,"Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, Ruslan Salakhutdinov",2019/1/9,arXiv preprint arXiv:1901.02860,,,,,"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",1118회,"Transformer-xl: Attentive language models beyond a fixed-length context
Z Dai, Z Yang, Y Yang, J Carbonell, QV Le… - arXiv preprint arXiv:1901.02860, 2019
1118회 인용 관련 학술자료 전체 10개의 버전",,,,
Aligning books and movies: Towards story-like visual explanations by watching movies and reading books,"Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler",2015,,,,19-27,,"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",1070회,"Aligning books and movies: Towards story-like visual explanations by watching movies and reading books
Y Zhu, R Kiros, R Zemel, R Salakhutdinov, R Urtasun… - Proceedings of the IEEE international conference on …, 2015
1070회 인용 관련 학술자료 전체 16개의 버전",Proceedings of the IEEE international conference on computer vision,,,
Deep sets,"Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, Alexander Smola",2017/3/10,arXiv preprint arXiv:1703.06114,,,,,"We study the problem of designing models for machine learning tasks defined on\emph {sets}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics\cite {poczos13aistats}, to anomaly detection in piezometer data of embankment dams\cite {Jung15Exploration}, to cosmology\cite {Ntampaka16Dynamical, Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.",934회,"Deep sets
M Zaheer, S Kottur, S Ravanbakhsh, B Poczos… - arXiv preprint arXiv:1703.06114, 2017
934회 인용 관련 학술자료 전체 4개의 버전",,,,
Evaluation methods for topic models,"Hanna M Wallach, Iain Murray, Ruslan Salakhutdinov, David Mimno",2009/6/14,,,,1105-1112,,"A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.",904회,"Evaluation methods for topic models
HM Wallach, I Murray, R Salakhutdinov, D Mimno - Proceedings of the 26th annual international …, 2009
904회 인용 관련 학술자료 전체 26개의 버전",,Proceedings of the 26th annual international conference on machine learning,,
Revisiting Semi-Supervised Learning with Graph Embeddings,"Zhilin Yang, William Cohen, Ruslan Salakhutdinov",2016/3/29,,,,,,"We present a semi-supervised learning framework based on graph embeddings. Given a graph between instances, we train an embedding for each instance to jointly predict the class label and the neighborhood context in the graph. We develop both transductive and inductive variants of our method. In the transductive variant of our method, the class labels are determined by both the learned embeddings and input feature vectors, while in the inductive variant, the embeddings are defined as a parametric function of the feature vectors, so predictions can be made on instances not seen during training. On a large and diverse set of benchmark tasks, including text classification, distantly supervised entity extraction, and entity classification, we show improved performance over many of the existing models.",847회,"Revisiting semi-supervised learning with graph embeddings
Z Yang, W Cohen, R Salakhudinov - International conference on machine learning, 2016
847회 인용 관련 학술자료 전체 13개의 버전",International Conference on Machine Learning (ICML),,,
Importance weighted autoencoders,"Yuri Burda, Roger Grosse, Ruslan Salakhutdinov",2015/9/1,,,,,,"The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.",772회,"Importance weighted autoencoders
Y Burda, R Grosse, R Salakhutdinov - arXiv preprint arXiv:1509.00519, 2015
772회 인용 관련 학술자료 전체 3개의 버전",International Conference on Learning Representations (ICLR),,,
Multimodal Neural Language Models.,"Ryan Kiros, Ruslan Salakhutdinov, Richard S Zemel",2014/6/21,,14,,595-603,,"We introduce two multimodal neural language models: models of natural language that can be conditioned on other modalities. An image-text multimodal neural language model can be used to retrieve images given complex sentence queries, retrieve phrase descriptions given image queries, as well as generate text conditioned on images. We show that in the case of image-text modelling we can jointly learn word representations and image features by training our models together with a convolutional network. Unlike many of the existing methods, our approach can generate sentence descriptions for images without the use of templates, structured prediction, and/or syntactic trees. While we focus on image-text modelling, our algorithms can be easily applied to other modalities such as audio.",640회,"Multimodal neural language models
R Kiros, R Salakhutdinov, R Zemel - International conference on machine learning, 2014
640회 인용 관련 학술자료 전체 21개의 버전",International Conference on Machine Learning (ICML),,,
Toward controlled generation of text,"Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P Xing",2017/7/17,,,,1587-1596,PMLR,"Generic generation and manipulation of text is challenging and has limited success compared to recent deep generative modeling in visual domain. This paper aims at generating plausible text sentences, whose attributes are controlled by learning disentangled latent representations with designated semantics. We propose a new neural generative model which combines variational auto-encoders (VAEs) and holistic attribute discriminators for effective imposition of semantic structures. The model can alternatively be seen as enhancing VAEs with the wake-sleep algorithm for leveraging fake samples as extra training data. With differentiable approximation to discrete text samples, explicit constraints on independent attribute controls, and efficient collaborative learning of generator and discriminators, our model learns interpretable representations from even only word annotations, and produces short sentences with desired attributes of sentiment and tenses. Quantitative experiments using trained classifiers as evaluators validate the accuracy of sentence and attribute generation.",586회,"Toward controlled generation of text
Z Hu, Z Yang, X Liang, R Salakhutdinov, EP Xing - International Conference on Machine Learning, 2017
586회 인용 관련 학술자료 전체 9개의 버전",International Conference on Machine Learning,,,
Replicated softmax: an undirected topic model,"Ruslan Salakhutdinov, Geoffrey Hinton",2009,,,,1607-1614,,"We introduce a two-layer undirected graphical model, called a “Replicated Softmax”, that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.",580회,"Replicated softmax: an undirected topic model
GE Hinton, RR Salakhutdinov - Advances in neural information processing systems, 2009
580회 인용 관련 학술자료 전체 14개의 버전",Advances in neural information processing systems,,,
Action recognition using visual attention,"Shikhar Sharma, Ryan Kiros, Ruslan Salakhutdinov",2015/11/12,,,,,,"We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.",578회,"Action recognition using visual attention
S Sharma, R Kiros, R Salakhutdinov - arXiv preprint arXiv:1511.04119, 2015
578회 인용 관련 학술자료 전체 9개의 버전","International Conference on Learning Representations (ICLR) , workshop",,,
Hamming distance metric learning,"Mohammad Norouzi, David J Fleet, Russ R Salakhutdinov",2012,,,,1061-1069,,"Motivated by large-scale multimedia applications we propose to learn mappings from high-dimensional data to binary codes that preserve semantic similarity. Binary codes are well suited to large-scale applications as they are storage efficient and permit exact sub-linear kNN search. The framework is applicable to broad families of mappings, and uses a flexible form of triplet ranking loss. We overcome discontinuous optimization of the discrete mappings by minimizing a piecewise-smooth upper bound on empirical loss, inspired by latent structural SVMs. We develop a new loss-augmented inference algorithm that is quadratic in the code length. We show strong retrieval performance on CIFAR-10 and MNIST, with promising classification results using no more than kNN on the binary codes.",564회,"Hamming distance metric learning
M Norouzi, DJ Fleet, RR Salakhutdinov - Advances in neural information processing systems, 2012
564회 인용 관련 학술자료 전체 13개의 버전",Advances in neural information processing systems,,,
Deep learning for neuroimaging: a validation study,"Sergey M Plis, Devon R Hjelm, Ruslan Salakhutdinov, Elena A Allen, Henry J Bockholt, Jeffrey D Long, Hans J Johnson, Jane S Paulsen, Jessica A Turner, Vince D Calhoun",2014/8/20,Frontiers in neuroscience,8,,229,Frontiers,"Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. These methods include deep belief networks and their building block the restricted Boltzmann machine. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.",563회,"Deep learning for neuroimaging: a validation study
SM Plis, DR Hjelm, R Salakhutdinov, EA Allen… - Frontiers in neuroscience, 2014
563회 인용 관련 학술자료 전체 29개의 버전",,,,
One shot learning of simple visual concepts,"Brenden Lake, Ruslan Salakhutdinov, Jason Gross, Joshua Tenenbaum",2011,Proceedings of the annual meeting of the cognitive science society,33,33,,,"People can learn visual concepts from just one example, but it remains a mystery how this is accomplished. Many authors have proposed that transferred knowledge from more familiar concepts is a route to one shot learning, but what is the form of this abstract knowledge? One hypothesis is that the sharing of parts is core to one shot learning, and we evaluate this idea in the domain of handwritten characters, using a massive new dataset. These simple visual concepts have a rich internal part structure, yet they are particularly tractable for computational models. We introduce a generative model of how characters are composed from strokes, where knowledge from previous characters helps to infer the latent strokes in novel characters. The stroke model outperforms a competing stateof-the-art character model on a challenging one shot learning task, and it provides a good fit to human perceptual data.",529회,"One shot learning of simple visual concepts
B Lake, R Salakhutdinov, J Gross, J Tenenbaum - Proceedings of the annual meeting of the cognitive …, 2011
529회 인용 관련 학술자료 전체 18개의 버전",,,,
Efficient Learning of Deep Boltzmann Machines.,"Ruslan Salakhutdinov, Hugo Larochelle",2010/5,,,,693-700,,"We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",528회,"Efficient learning of deep Boltzmann machines
R Salakhutdinov, H Larochelle - Proceedings of the thirteenth international conference …, 2010
408회 인용 관련 학술자료 전체 19개의 버전
Proceedings of the thirteenth international conference on artificial intelligence and statistics*
X Glorot, Y Bengio, YW Teh, M Titterington - PMLR, 2010
123회 인용 관련 학술자료",International Conference on Artificial Intelligence and Statistics (AISTATS),,,
Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure.,"Ruslan Salakhutdinov, Geoffrey E Hinton",2007/3/21,,,,412-419,,"We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classification performs well. We also show how the non-linear transformation can be improved using unlabeled data. Our method achieves a much lower error rate than Support Vector Machines or standard backpropagation on a widely used version of the MNIST handwritten digit recognition task. If some of the dimensions of the low-dimensional feature space are not used for nearest neighbor classification, our method uses these dimensions to explicitly represent transformations of the digits that do not affect their identity.",527회,"Learning a nonlinear embedding by preserving class neighbourhood structure
R Salakhutdinov, G Hinton - Artificial Intelligence and Statistics, 2007
527회 인용 관련 학술자료 전체 19개의 버전",International Conference on Artificial Intelligence and Statistics (AISTATS),,,
On the quantitative analysis of deep belief networks,"Ruslan Salakhutdinov, Iain Murray",2008/7/5,,,,872-879,,"Deep Belief Networks (DBN's) are generative models that contain many layers of hidden variables. Efficient greedy algorithms for learning and approximate inference have allowed these models to be applied successfully in many application domains. The main building block of a DBN is a bipartite undirected graphical model called a restricted Boltzmann machine (RBM). Due to the presence of the partition function, model selection, complexity control, and exact maximum likelihood learning in RBM's are intractable. We show that Annealed Importance Sampling (AIS) can be used to efficiently estimate the partition function of an RBM, and we present a novel AIS scheme for comparing RBM's with different architectures. We further show how an AIS estimator, along with approximate inference, can be used to estimate a lower bound on the log-probability that a DBN model with multiple hidden layers assigns to the test …",486회,"On the quantitative analysis of deep belief networks
R Salakhutdinov, I Murray - Proceedings of the 25th international conference on …, 2008
486회 인용 관련 학술자료 전체 21개의 버전",,Proceedings of the 25th international conference on Machine learning,,
An efficient learning procedure for deep Boltzmann machines,"Ruslan Salakhutdinov, Geoffrey Hinton",2012/8,Neural computation,24,8,1967-2006,MIT Press,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects …",483회,"An efficient learning procedure for deep Boltzmann machines
R Salakhutdinov, G Hinton - Neural computation, 2012
483회 인용 관련 학술자료 전체 24개의 버전",,,,
"Hotpotqa: A dataset for diverse, explainable multi-hop question answering","Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, Christopher D Manning",2018/9/25,arXiv preprint arXiv:1809.09600,,,,,"Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features:(1) the questions require finding and reasoning over multiple supporting documents to answer;(2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas;(3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions;(4) we offer a new type of factoid comparison questions to test QA systems' ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",450회,"Hotpotqa: A dataset for diverse, explainable multi-hop question answering
Z Yang, P Qi, S Zhang, Y Bengio, WW Cohen… - arXiv preprint arXiv:1809.09600, 2018
450회 인용 관련 학술자료 전체 9개의 버전",,,,
Deep kernel learning,"Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P Xing",2015/11/6,,,,,,"We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric flexibility of kernel methods. Specifically, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with benefits in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O (n) for n training points, and predictions cost O (1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with flexible kernel learning models, and stand-alone deep architectures.",400회,"Deep kernel learning
AG Wilson, Z Hu, R Salakhutdinov, EP Xing - Artificial intelligence and statistics, 2016
400회 인용 관련 학술자료 전체 12개의 버전",International Conference on Artificial Intelligence and Statistics (AISTATS),,,
Actor-mimic: Deep multitask and transfer reinforcement learning,"Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov",2015/11/19,,,,,,"The ability to act in multiple environments and transfer previous knowledge to new situations can be considered a critical aspect of any intelligent agent. Towards this goal, we define a novel method of multitask and transfer learning that enables an autonomous agent to learn how to behave in multiple tasks simultaneously, and then generalize its knowledge to new domains. This method, termed"" Actor-Mimic"", exploits the use of deep reinforcement learning and model compression techniques to train a single policy network that learns how to act in a set of distinct tasks by using the guidance of several expert teachers. We then show that the representations learnt by the deep policy network are capable of generalizing to new tasks with no prior expert guidance, speeding up learning in novel environments. Although our method can in general be applied to a wide range of problems, we use Atari games as a testing environment to demonstrate these methods.",372회,"Actor-mimic: Deep multitask and transfer reinforcement learning
E Parisotto, JL Ba, R Salakhutdinov - arXiv preprint arXiv:1511.06342, 2015
372회 인용 관련 학술자료 전체 3개의 버전",International Conference on Learning Representations (ICLR),,,
Learning deep generative models,Ruslan Salakhutdinov,2015/4/10,,2,,361-385,Annual Reviews,"Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many artificial intelligence–related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. In this article, we review several popular deep learning models, including deep belief networks and deep Boltzmann machines. We show that (a) these deep generative models, which contain many layers of latent variables and millions of parameters, can be learned efficiently, and (b) the learned high-level feature representations can be successfully applied in many application domains, including visual object recognition, information retrieval, classification, and regression tasks.",348회,"Learning deep generative models
R Salakhutdinov - Annual Review of Statistics and Its Application, 2015
348회 인용 관련 학술자료 전체 24개의 버전",,,Annual Review of Statistics and Its Application,
Multimodal learning with deep Boltzmann machines.,"Nitish Srivastava, Ruslan Salakhutdinov",2014/10/1,J. Mach. Learn. Res.,15,1,2949-2980,,"Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bi-modal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",348회,"Multimodal learning with deep Boltzmann machines.
N Srivastava, R Salakhutdinov - J. Mach. Learn. Res., 2014
348회 인용 관련 학술자료 전체 15개의 버전",,,,
Predicting deep zero-shot convolutional neural networks using textual descriptions,"Jimmy Lei Ba, Kevin Swersky, Sanja Fidler",2015,,,,4247-4255,,"One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo-attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end using the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.",347회,"Predicting deep zero-shot convolutional neural networks using textual descriptions
J Lei Ba, K Swersky, S Fidler - Proceedings of the IEEE International Conference on …, 2015
347회 인용 관련 학술자료 전체 12개의 버전",Proceedings of the IEEE International Conference on Computer Vision,,,
Gated-attention readers for text comprehension,"Bhuwan Dhingra, Hanxiao Liu, Zhilin Yang, William W Cohen, Ruslan Salakhutdinov",2016/6/5,arXiv preprint arXiv:1606.01549,,,,,"In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN\& Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at this https URL.",341회,"Gated-attention readers for text comprehension
B Dhingra, H Liu, Z Yang, WW Cohen, R Salakhutdinov - arXiv preprint arXiv:1606.01549, 2016
341회 인용 관련 학술자료 전체 10개의 버전",,,,
Good semi-supervised learning that requires a bad gan,"Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, Ruslan Salakhutdinov",2017/5/27,arXiv preprint arXiv:1705.09783,,,,,"Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically, we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets.",307회,"Good semi-supervised learning that requires a bad gan
Z Dai, Z Yang, F Yang, WW Cohen, R Salakhutdinov - arXiv preprint arXiv:1705.09783, 2017
307회 인용 관련 학술자료 전체 8개의 버전",,,,
Learning to share visual appearance for multiclass object detection,"Ruslan Salakhutdinov, Antonio Torralba, Josh Tenenbaum",2011/6/20,,,,1481-1488,IEEE,"We present a hierarchical classification model that allows rare objects to borrow statistical strength from related objects that have many training examples. Unlike many of the existing object detection and recognition systems that treat different classes as unrelated entities, our model learns both a hierarchy for sharing visual appearance across 200 object categories and hierarchical parameters. Our experimental results on the challenging object localization and detection task demonstrate that the proposed model substantially improves the accuracy of the standard single object detectors that ignore hierarchical structure altogether.",296회,"Learning to share visual appearance for multiclass object detection
R Salakhutdinov, A Torralba, J Tenenbaum - CVPR 2011, 2011
296회 인용 관련 학술자료 전체 11개의 버전",CVPR 2011,,,
On exact computation with an infinitely wide neural net,"Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang",2019/4/26,arXiv preprint arXiv:1904.11955,,,,,"How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its width---namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers---is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width.
The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for the performance of a pure kernel-based method on CIFAR-10, being higher than the methods reported in [Novak et al., 2019], and only lower than the performance of the corresponding finite deep net architecture (once batch normalization, etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.",289회,"On exact computation with an infinitely wide neural net
S Arora, SS Du, W Hu, Z Li, R Salakhutdinov, R Wang - arXiv preprint arXiv:1904.11955, 2019
289회 인용 관련 학술자료 전체 6개의 버전",,,,
"Encode, Review, and Decode: Reviewer Module for Caption Generation","Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W Cohen",2016/5/25,NIPS 2016,,,,,,280회,"Review networks for caption generation*
Z Yang, Y Yuan, Y Wu, R Salakhutdinov, WW Cohen - arXiv preprint arXiv:1605.07912, 2016
210회 인용 관련 학술자료 전체 7개의 버전
Encode, review, and decode: Reviewer module for caption generation*
Z Wu, R Cohen - arXiv preprint arXiv:1605.07912, 2016
72회 인용 관련 학술자료",,,,
Generating images from captions with attention,"Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, Ruslan Salakhutdinov",2015/11/9,,,,,,"Motivated by the recent progress in generative models, we introduce a model that generates images from natural language descriptions. The proposed model iteratively draws patches on a canvas, while attending to the relevant words in the description. After training on Microsoft COCO, we compare our model with several baseline generative models on image generation and retrieval tasks. We demonstrate that our model produces higher quality samples than other approaches and generates images with novel scene compositions corresponding to previously unseen captions in the dataset.",278회,"Generating images from captions with attention
E Mansimov, E Parisotto, JL Ba, R Salakhutdinov - arXiv preprint arXiv:1511.02793, 2015
278회 인용 관련 학술자료 전체 15개의 버전",International Conference on Learning Representations (ICLR),,,
Transfer learning for sequence tagging with hierarchical recurrent networks,"Zhilin Yang, Ruslan Salakhutdinov, William W Cohen",2017/3/18,arXiv preprint arXiv:1703.06345,,,,,"Recent papers have shown that neural networks obtain state-of-the-art performance on several different sequence tagging tasks. One appealing property of such systems is their generality, as excellent performance can be achieved with a unified architecture and without task-specific feature engineering. However, it is unclear if such systems can be used for tasks without large amounts of training data. In this paper we explore the problem of transfer learning for neural sequence taggers, where a source task with plentiful annotations (eg, POS tagging on Penn Treebank) is used to improve performance on a target task with fewer available annotations (eg, POS tagging for microblogs). We examine the effects of transfer learning for deep hierarchical recurrent networks across domains, applications, and languages, and show that significant improvement can often be obtained. These improvements lead to improvements over the current state-of-the-art on several well-studied tasks.",275회,"Transfer learning for sequence tagging with hierarchical recurrent networks
Z Yang, R Salakhutdinov, WW Cohen - arXiv preprint arXiv:1703.06345, 2017
275회 인용 관련 학술자료 전체 7개의 버전",,,,
Modelling relational data using bayesian clustered tensor factorization,"Ilya Sutskever, Ruslan R Salakhutdinov, Joshua B Tenenbaum",2009,,,,,Neural Information Processing Systems Foundation,"We consider the problem of learning probabilistic models for complex relational structures between various types of objects. A model can help us ""understand"" a dataset of relational facts in at least two ways, by finding interpretable structure in the data, and by supporting predictions, or inferences about whether particular unobserved relations are likely to be true. Often there is a tradeoff between these two aims: cluster-based models yield more easily interpretable representations, while factorization-based approaches have given better predictive performance on large data sets. We introduce the Bayesian Clustered Tensor Factorization (BCTF) model, which embeds a factorized representation of relations in a nonparametric Bayesian clustering framework. Inference is fully Bayesian but scales well to large data sets. The model simultaneously discovers interpretable clusters and yields predictive performance that matches or beats previous probabilistic models for relational data.",275회,"Modelling relational data using bayesian clustered tensor factorization
I Sutskever, RR Salakhutdinov, JB Tenenbaum - 2009
275회 인용 관련 학술자료 전체 20개의 버전",,,,
Breaking the softmax bottleneck: A high-rank RNN language model,"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W Cohen",2017/11/10,arXiv preprint arXiv:1711.03953,,,,,"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",265회,"Breaking the softmax bottleneck: A high-rank RNN language model
Z Yang, Z Dai, R Salakhutdinov, WW Cohen - arXiv preprint arXiv:1711.03953, 2017
265회 인용 관련 학술자료 전체 4개의 버전",,,,
Semantic hashing,"Ruslan Salakhutdinov, Geoffrey Hinton",2007/7,,1,1,8,,"We show how to learn a deep graphical model of the word-count vectors obtained from a large set of documents. The values of the latent variables in the deepest layer are easy to infer and give a much better representation of each document than Latent Semantic Analysis. When the deepest layer is forced to use a small number of binary variables (eg 32), the graphical model performs “semantic hashing”: Documents are mapped to memory addresses in such a way that semantically similar documents are located at nearby addresses. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than locality sensitive hashing, which is the fastest current method. By using semantic hashing to filter the documents given to TF-IDF, we achieve higher accuracy than applying TF-IDF to the entire document set.",257회,"Semantic hashing
R Salakhutdinov, G Hinton - RBM, 2007
257회 인용 관련 학술자료 전체 17개의 버전",SIGIR workshop on Information Retrieval and applications of Graphical Models,,,
Multi-task neural networks for QSAR predictions,"George E Dahl, Navdeep Jaitly, Ruslan Salakhutdinov",2014/6/4,arXiv preprint arXiv:1406.1231,,,,,"Although artificial neural networks have occasionally been used for Quantitative Structure-Activity/Property Relationship (QSAR/QSPR) studies in the past, the literature has of late been dominated by other machine learning techniques such as random forests. However, a variety of new neural net techniques along with successful applications in other domains have renewed interest in network approaches. In this work, inspired by the winning team's use of neural networks in a recent QSAR competition, we used an artificial neural network to learn a function that predicts activities of compounds for multiple assays at the same time. We conducted experiments leveraging recent methods for dealing with overfitting in neural networks as well as other tricks from the neural networks literature. We compared our methods to alternative methods reported to perform well on these tasks and found that our neural net methods provided superior performance.",253회,"Multi-task neural networks for QSAR predictions
GE Dahl, N Jaitly, R Salakhutdinov - arXiv preprint arXiv:1406.1231, 2014
253회 인용 관련 학술자료 전체 8개의 버전",,,,
Learning with hierarchical-deep models,"Ruslan Salakhutdinov, Joshua B Tenenbaum, Antonio Torralba",2012/12/20,IEEE transactions on pattern analysis and machine intelligence,35,8,1958-1971,IEEE,"We introduce HD (or “Hierarchical-Deep”) models, a new compositional learning architecture that integrates deep learning models with structured hierarchical Bayesian (HB) models. Specifically, we show how we can learn a hierarchical Dirichlet process (HDP) prior over the activities of the top-level features in a deep Boltzmann machine (DBM). This compound HDP-DBM model learns to learn novel concepts from very few training example by learning low-level generic features, high-level features that capture correlations among low-level features, and a category hierarchy for sharing priors over the high-level features that are typical of different kinds of concepts. We present efficient learning and inference algorithms for the HDP-DBM model and show that it is able to learn new concepts from very few examples on CIFAR-100 object recognition, handwritten character recognition, and human motion capture datasets.",251회,"Learning with hierarchical-deep models
R Salakhutdinov, JB Tenenbaum, A Torralba - IEEE transactions on pattern analysis and machine …, 2012
251회 인용 관련 학술자료 전체 15개의 버전",,,,
Collaborative filtering in a non-uniform world: Learning with the weighted trace norm,"Ruslan Salakhutdinov, Nathan Srebro",2010/2/14,arXiv preprint arXiv:1002.2780,,,,,We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.,243회,"Collaborative filtering in a non-uniform world: Learning with the weighted trace norm
R Salakhutdinov, N Srebro - arXiv preprint arXiv:1002.2780, 2010
243회 인용 관련 학술자료 전체 18개의 버전",,,,
Improved variational autoencoders for text modeling using dilated convolutions,"Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, Taylor Berg-Kirkpatrick",2017/7/17,,,,3881-3890,PMLR,"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",241회,"Improved variational autoencoders for text modeling using dilated convolutions
Z Yang, Z Hu, R Salakhutdinov, T Berg-Kirkpatrick - International conference on machine learning, 2017
241회 인용 관련 학술자료 전체 6개의 버전",International conference on machine learning,,,
Style transfer through back-translation,"Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov, Alan W Black",2018/4/24,arXiv preprint arXiv:1804.09000,,,,,"Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.",235회,"Style transfer through back-translation
S Prabhumoye, Y Tsvetkov, R Salakhutdinov, AW Black - arXiv preprint arXiv:1804.09000, 2018
235회 인용 관련 학술자료 전체 11개의 버전",,,,
Using deep belief nets to learn covariance kernels for Gaussian processes,"Ruslan Salakhutdinov, Geoffrey Hinton",2008,,,,1249-1256,,"We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We first learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classification can then be further improved by using backpropagation through the DBN to discriminatively fine-tune the covariance kernel.",230회,"Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes.
R Salakhutdinov, GE Hinton - NIPS, 2007
230회 인용 관련 학술자료 전체 19개의 버전",Advances in neural information processing systems,,,
Discriminative Transfer Learning with Tree-based Priors.,"Nitish Srivastava, Ruslan Salakhutdinov",2013/6/3,NIPS,3,4,8,,"This paper proposes a way of improving classification performance for classes which have very few training examples. The key idea is to discover classes which are similar and transfer knowledge among them. Our method organizes the classes into a tree hierarchy. The tree structure can be used to impose a prior over classification parameters. We show that these priors can be combined with discriminative models such as deep neural networks. Our method benefits from the power of discriminative training of deep neural networks, at the same time using treebased priors over classification parameters. We also propose an algorithm for learning the underlying tree structure. This gives the model some flexibility to tune the tree so that the tree is pertinent to task being solved. We show that the model can transfer knowledge across related classes using fixed trees. Moreover, it can learn new meaningful trees usually leading to improved performance. Our method achieves state-of-the-art classification results on the CIFAR-100 image dataset and the MIR Flickr multimodal dataset.",224회,"Discriminative Transfer Learning with Tree-based Priors.
N Srivastava, R Salakhutdinov - NIPS, 2013
224회 인용 관련 학술자료 전체 10개의 버전
Supplementary Material for Discriminative Transfer Learning with Tree-based Priors*
N Srivastava, R Salakhutdinov
관련 학술자료 전체 3개의 버전",,,,
Spatially adaptive computation time for residual networks,"Michael Figurnov, Maxwell D Collins, Yukun Zhu, Li Zhang, Jonathan Huang, Dmitry Vetrov, Ruslan Salakhutdinov",2017,,,,1039-1048,,"This paper proposes a deep learning architecture based on Residual Network that dynamically adjusts the number of executed layers for the regions of the image. This architecture is end-to-end trainable, deterministic and problem-agnostic. It is therefore applicable without any modifications to a wide range of computer vision problems such as image classification, object detection and image segmentation. We present experimental results showing that this model improves the computational efficiency of Residual Networks on the challenging ImageNet classification and COCO object detection datasets. Additionally, we evaluate the computation time maps on the visual saliency dataset cat2000 and find that they correlate surprisingly well with human eye fixation positions.",216회,"Spatially adaptive computation time for residual networks
M Figurnov, MD Collins, Y Zhu, L Zhang, J Huang… - Proceedings of the IEEE Conference on Computer …, 2017
216회 인용 관련 학술자료 전체 8개의 버전",Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,,,
Path-sgd: Path-normalized optimization in deep neural networks,"Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro",2015/6/8,arXiv preprint arXiv:1506.02617,,,,,"We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights. We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization. Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.",211회,"Path-sgd: Path-normalized optimization in deep neural networks
B Neyshabur, R Salakhutdinov, N Srebro - arXiv preprint arXiv:1506.02617, 2015
211회 인용 관련 학술자료 전체 5개의 버전",,,,
Exploiting image-trained cnn architectures for unconstrained video classification,"Shengxin Zha, Florian Luisier, Walter Andrews, Nitish Srivastava, Ruslan Salakhutdinov",2015/3/13,,,,,,"We conduct an in-depth exploration of different strategies for doing event detection in videos using convolutional neural networks (CNNs) trained for image classification. We study different ways of performing spatial and temporal pooling, feature normalization, choice of CNN layers as well as choice of classifiers. Making judicious choices along these dimensions led to a very significant increase in performance over more naive approaches that have been used till now. We evaluate our approach on the challenging TRECVID MED'14 dataset with two popular CNN architectures pretrained on ImageNet. On this MED'14 dataset, our methods, based entirely on image-trained CNN features, can outperform several state-of-the-art non-CNN models. Our proposed late fusion of CNN-and motion-based features can further increase the mean average precision (mAP) on MED'14 from 34.95% to 38.74%. The fusion approach achieves the state-of-the-art classification performance on the challenging UCF-101 dataset.",211회,"Exploiting image-trained CNN architectures for unconstrained video classification
S Zha, F Luisier, W Andrews, N Srivastava… - arXiv preprint arXiv:1503.04144, 2015
211회 인용 관련 학술자료 전체 7개의 버전",BMVC,,,
One-shot learning by inverting a compositional causal process,"Brenden M Lake, Ruslan Salakhutdinov, Joshua B Tenenbaum",2013,,,,,"Neural Information Processing Systems Foundation, Inc.","People can learn a new visual class from just one example, yet machine learning algorithms typically require hundreds or thousands of examples to tackle the same problems. Here we present a Hierarchical Bayesian model based on compositionality and causality that can learn a wide range of natural (although simple) visual concepts, generalizing in human-like ways from just one image. We evaluated performance on a challenging one-shot classification task, where our model achieved a human-level error rate while substantially outperforming two deep learning models. We also used a visual Turing test ""to show that our model produces human-like performance on other conceptual tasks, including generating new examples and parsing.""",207회,"One-shot learning by inverting a compositional causal process
BM Lake, R Salakhutdinov, JB Tenenbaum - 2013
207회 인용 관련 학술자료 전체 11개의 버전",,,,
The more you know: Using knowledge graphs for image classification,"Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta",2016/12/14,arXiv preprint arXiv:1612.04844,,,,,"One characteristic that sets humans apart from modern learning-based computer vision algorithms is the ability to acquire knowledge about the world and use that knowledge to reason about the visual world. Humans can learn about the characteristics of objects and the relationships that occur between them to learn a large variety of visual concepts, often with few examples. This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification. We build on recent work on end-to-end learning on graphs, introducing the Graph Search Neural Network as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline. We show in a number of experiments that our method outperforms standard neural network baselines for multi-label classification.",203회,"The more you know: Using knowledge graphs for image classification
K Marino, R Salakhutdinov, A Gupta - arXiv preprint arXiv:1612.04844, 2016
203회 인용 관련 학술자료 전체 6개의 버전",,,,
Modeling documents with deep boltzmann machines,"Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E Hinton",2013/9/26,,,,,,"We introduce a Deep Boltzmann Machine model suitable for modeling and extracting latent semantic representations from a large unstructured collection of documents. We overcome the apparent difficulty of training a DBM with judicious parameter tying. This parameter tying enables an efficient pretraining algorithm and a state initialization scheme that aids inference. The model can be trained just as efficiently as a standard Restricted Boltzmann Machine. Our experiments show that the model assigns better log probability to unseen data than the Replicated Softmax model. Features extracted from our model outperform LDA, Replicated Softmax, and DocNADE models on document retrieval and document classification tasks.",198회,"Modeling documents with deep boltzmann machines
N Srivastava, RR Salakhutdinov, GE Hinton - arXiv preprint arXiv:1309.6865, 2013
198회 인용 관련 학술자료 전체 23개의 버전",Uncertainty in Artificial Intelligence (UAI),,,
On the quantitative analysis of decoder-based generative models,"Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, Roger Grosse",2016/11/14,arXiv preprint arXiv:1611.04273,,,,,"The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at this https URL. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.",195회,"On the quantitative analysis of decoder-based generative models
Y Wu, Y Burda, R Salakhutdinov, R Grosse - arXiv preprint arXiv:1611.04273, 2016
195회 인용 관련 학술자료 전체 6개의 버전",,,,
Learning representations for multimodal data with deep belief nets,"Nitish Srivastava, Ruslan Salakhutdinov",2012/7,International conference on machine learning workshop,79,,3,,We propose a Deep Belief Network architecture for learning a joint representation of multimodal data. The model defines a probability distribution over the space of multimodal inputs and allows sampling from the conditional distributions over each data modality. This makes it possible for the model to create a multimodal representation even when some data modalities are missing. Our experimental results on bi-modal data consisting of images and text show that the Multimodal DBN can learn a good generative model of the joint space of image and text inputs that is useful for filling in missing data so it can be used both for image annotation and image retrieval. We further demonstrate that using the representation discovered by the Multimodal DBN our model can significantly outperform SVMs and LDA on discriminative tasks.,194회,"Learning representations for multimodal data with deep belief nets
N Srivastava, R Salakhutdinov - International conference on machine learning …, 2012
194회 인용 관련 학술자료 전체 6개의 버전",,,,
Optimization with EM and expectation-conjugate-gradient,"Ruslan Salakhutdinov, Sam Roweis, Zoubin Ghahramani",2003/8/21,,,,672-679,,"We show a close relationship between the Expectation-Maximization (EM) algorithm and direct optimization algorithms such as gradientbased methods for parameter learning. We identify analytic conditions under which EM exhibits Newton-like behavior, and conditions under which it possesses poor, first-order convergence. Based on this analysis, we propose two novel algorithms for maximum likelihood estimation of latent variable models, and report empirical results showing that, as predicted by theory, the proposed new algorithms can substantially outperform standard EM in terms of speed of convergence in certain cases.",190회,"Optimization with EM and expectation-conjugate-gradient
R Salakhutdinov, ST Roweis, Z Ghahramani - Proceedings of the 20th International Conference on …, 2003
190회 인용 관련 학술자료 전체 27개의 버전",International Conference on Machine Learning (ICML),,,
Robust boltzmann machines for recognition and denoising,"Yichuan Tang, Ruslan Salakhutdinov, Geoffrey Hinton",2012/6/16,,,,2264-2271,IEEE,"While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases.",187회,"Robust boltzmann machines for recognition and denoising
Y Tang, R Salakhutdinov, G Hinton - 2012 IEEE conference on computer vision and pattern …, 2012
187회 인용 관련 학술자료 전체 15개의 버전",2012 IEEE conference on computer vision and pattern recognition,,,
Multi-task cross-lingual sequence tagging from scratch,"Zhilin Yang, Ruslan Salakhutdinov, William Cohen",2016/3/20,arXiv preprint arXiv:1603.06270,,,,,"We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and cross-lingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.",186회,"Multi-task cross-lingual sequence tagging from scratch
Z Yang, R Salakhutdinov, W Cohen - arXiv preprint arXiv:1603.06270, 2016
186회 인용 관련 학술자료 전체 2개의 버전",,,,
Neural map: Structured memory for deep reinforcement learning,"Emilio Parisotto, Ruslan Salakhutdinov",2017/2/27,arXiv preprint arXiv:1702.08360,,,,,"A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.",176회,"Neural map: Structured memory for deep reinforcement learning
E Parisotto, R Salakhutdinov - arXiv preprint arXiv:1702.08360, 2017
176회 인용 관련 학술자료 전체 3개의 버전",,,,
segdeepm: Exploiting segmentation and context in deep neural networks for object detection,"Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, Sanja Fidler",2015,,,,4703-4711,,"In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL VOC 2010, and 1.4% over the current state-of-the-art, demonstrating the power of our approach.",168회,"segdeepm: Exploiting segmentation and context in deep neural networks for object detection
Y Zhu, R Urtasun, R Salakhutdinov, S Fidler - Proceedings of the IEEE Conference on Computer …, 2015
168회 인용 관련 학술자료 전체 18개의 버전",Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,,,
Practical large-scale optimization for max-norm regularization,"Jason Lee, Benjamin Recht, Ruslan R Salakhutdinov, Nathan Srebro, Joel A Tropp",2010,,,23,,Neural Information Processing Systems,"The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas.",166회,"Practical large-scale optimization for max-norm regularization
J Lee, B Recht, RR Salakhutdinov, N Srebro, JA Tropp - 2010
166회 인용 관련 학술자료 전체 23개의 버전",,,,
Transfer learning by borrowing examples for multiclass object detection,Joseph Jaewhan Lim,2012,,,,,,"Despite the recent trend of increasingly large datasets for object detection, there still exist many classes with few training examples. To overcome this lack of training data for certain classes, we propose a novel way of augmenting the training data for each class by borrowing and transforming examples from other classes. Our model learns which training instances from other classes to borrow and how to transform the borrowed examples so that they become more similar to instances from the target class. Our experimental results demonstrate that our new object detector, with borrowed and transformed examples, improves upon the current state-of-the-art detector on the challenging SUN09 object detection dataset.",152회,"Transfer learning by borrowing examples for multiclass object detection
JJ Lim - 2012
152회 인용 관련 학술자료 전체 16개의 버전",,,,Massachusetts Institute of Technology
Stochastic variational deep kernel learning,"Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P Xing",2016/11/1,arXiv preprint arXiv:1611.00336,,,,,"Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.",147회,"Stochastic variational deep kernel learning
AG Wilson, Z Hu, R Salakhutdinov, EP Xing - arXiv preprint arXiv:1611.00336, 2016
147회 인용 관련 학술자료 전체 6개의 버전",,,,
One-shot learning with a hierarchical nonparametric bayesian model,"Ruslan Salakhutdinov, Joshua Tenenbaum, Antonio Torralba",2012/6/27,,,,195-206,JMLR Workshop and Conference Proceedings,"We develop a hierarchical Bayesian model that learns categories from single training examples. The model transfers acquired knowledge from previously learned categories to a novel category, in the form of a prior over category means and variances. The model discovers how to group categories into meaningful super-categories that express different priors for new classes. Given a single example of a novel category, we can efficiently infer which super-category the novel category belongs to, and thereby estimate not only the new categories mean but also an appropriate similarity metric based on parameters inherited from the super-category. On MNIST and MSR Cambridge image datasets the model learns useful representations of novel categories based on just a single training example, and performs significantly better than simpler hierarchical Bayesian approaches. It can also discover new categories in a completely unsupervised fashion, given just one or a few examples.",145회,"One-shot learning with a hierarchical nonparametric bayesian model
R Salakhutdinov, J Tenenbaum, A Torralba - Proceedings of ICML Workshop on Unsupervised and …, 2012
145회 인용 관련 학술자료 전체 25개의 버전",Proceedings of ICML Workshop on Unsupervised and Transfer Learning,,,
On Multiplicative Integration with Recurrent Neural Networks,"Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, Ruslan Salakhutdinov",2016/6/21,NIPS 2016,,,,,"We introduce a general and simple structural design called Multiplicative Integration (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.",142회,"On multiplicative integration with recurrent neural networks
Y Wu, S Zhang, Y Zhang, Y Bengio, R Salakhutdinov - arXiv preprint arXiv:1606.06630, 2016
142회 인용 관련 학술자료 전체 9개의 버전",,,,
Architectural Complexity Measures of Recurrent Neural Networks,"Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan Salakhutdinov, Yoshua Bengio",2016/2/26,NIPS 2016,,,,,"In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs:(a) the recurrent depth, which captures the RNN's over-time nonlinear complexity,(b) the feedforward depth, which captures the local input-output nonlinearity (similar to the"" depth"" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.",138회,"Architectural complexity measures of recurrent neural networks
S Zhang, Y Wu, T Che, Z Lin, R Memisevic… - arXiv preprint arXiv:1602.08210, 2016
138회 인용 관련 학술자료 전체 8개의 버전",,,,
Improving neural networks by preventing co-adaptation of feature detectors. arXiv 2012,"Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, Ruslan R Salakhutdinov",,arXiv preprint arXiv:1207.0580,,,,,,133회,"Improving neural networks by preventing co-adaptation of feature detectors. arXiv 2012
GE Hinton, N Srivastava, A Krizhevsky, I Sutskever… - arXiv preprint arXiv:1207.0580
133회 인용 관련 학술자료",,,,
A better way to pretrain deep boltzmann machines,"Ruslan Salakhutdinov, Geoffrey Hinton",2012,,,,2447-2455,,"We describe how the pretraining algorithm for Deep Boltzmann Machines (DBMs) is related to the pretraining algorithm for Deep Belief Networks and we show that under certain conditions, the pretraining procedure improves the variational lower bound of a two-hidden-layer DBM. Based on this analysis, we develop a different method of pretraining DBMs that distributes the modelling work more evenly over the hidden layers. Our results on the MNIST and NORB datasets demonstrate that the new pretraining algorithm allows us to learn better generative models.",132회,"A better way to pretrain deep boltzmann machines
GE Hinton, RR Salakhutdinov - Advances in Neural Information Processing Systems, 2012
132회 인용 관련 학술자료 전체 14개의 버전",Advances in Neural Information Processing Systems,,,
Learning in Markov random fields using tempered transitions,Russ R Salakhutdinov,2009,Advances in neural information processing systems,22,,1598-1606,,"Markov random fields (MRF’s), or undirected graphical models, provide a powerful framework for modeling complex dependencies among random variables. Maximum likelihood learning in MRF’s is hard due to the presence of the global normalizing constant. In this paper we consider a class of stochastic approximation algorithms of the Robbins-Monro type that use Markov chain Monte Carlo to do approximate maximum likelihood learning. We show that using MCMC operators based on tempered transitions enables the stochastic approximation algorithm to better explore highly multimodal distributions, which considerably improves parameter estimates in large, densely-connected MRF’s. Our results on MNIST and NORB datasets demonstrate that we can successfully learn good generative models of high-dimensional, richly structured data that perform well on digit and object recognition tasks.",128회,"Learning in Markov random fields using tempered transitions
RR Salakhutdinov - Advances in neural information processing systems, 2009
128회 인용 관련 학술자료 전체 12개의 버전",,,,
Learning stochastic feedforward neural networks,"Charlie Tang, Russ R Salakhutdinov",2013,,,,530-538,,"Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classification tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (eg Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are not efficient and unsuitable for modeling real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables. A new Generalized EM training procedure using importance sampling allows us to efficiently learn complicated conditional distributions. Our model achieves superior performance on synthetic and facial expressions datasets compared to conditional Restricted Boltzmann Machines and Mixture Density Networks. In addition, the latent features of our model improves classification and can learn to generate colorful textures of objects.",127회,"Learning stochastic feedforward neural networks
C Tang, RR Salakhutdinov - Advances in Neural Information Processing Systems, 2013
127회 인용 관련 학술자료 전체 8개의 버전",Advances in Neural Information Processing Systems,,,
Open domain question answering using early fusion of knowledge bases and text,"Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, William W Cohen",2018/9/4,arXiv preprint arXiv:1809.00782,,,,,"Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting. Source code is available at this https URL.",124회,"Open domain question answering using early fusion of knowledge bases and text
H Sun, B Dhingra, M Zaheer, K Mazaitis… - arXiv preprint arXiv:1809.00782, 2018
124회 인용 관련 학술자료 전체 8개의 버전",,,,
Controllable text generation,"Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, Eric P Xing",2017/3,arXiv preprint arXiv:1703.00955,4,,,,,124회,"Controllable text generation
Z Hu, Z Yang, X Liang, R Salakhutdinov, EP Xing - arXiv preprint arXiv:1703.00955, 2017
124회 인용 관련 학술자료",,,,
Restricted Boltzmann machines for neuroimaging: an application in identifying intrinsic networks,"R Devon Hjelm, Vince D Calhoun, Ruslan Salakhutdinov, Elena A Allen, Tulay Adali, Sergey M Plis",2014/8/1,NeuroImage,96,,245-260,Academic Press,"Matrix factorization models are the current dominant approach for resolving meaningful data-driven features in neuroimaging data. Among them, independent component analysis (ICA) is arguably the most widely used for identifying functional networks, and its success has led to a number of versatile extensions to group and multimodal data. However there are indications that ICA may have reached a limit in flexibility and representational capacity, as the majority of such extensions are case-driven, custom-made solutions that are still contained within the class of mixture models. In this work, we seek out a principled and naturally extensible approach and consider a probabilistic model known as a restricted Boltzmann machine (RBM). An RBM separates linear factors from functional brain imaging data by fitting a probability distribution model to the data. Importantly, the solution can be used as a building block for …",122회,"Restricted Boltzmann machines for neuroimaging: an application in identifying intrinsic networks
RD Hjelm, VD Calhoun, R Salakhutdinov, EA Allen… - NeuroImage, 2014
122회 인용 관련 학술자료 전체 14개의 버전",,,,
Discovering binary codes for documents by learning deep generative models,"Geoffrey Hinton, Ruslan Salakhutdinov",2011/1,Topics in Cognitive Science,3,1,74-91,Blackwell Publishing Ltd,"We describe a deep generative model in which the lowest layer represents the word‐count vector of a document and the top layer represents a learned binary code for that document. The top two layers of the generative model form an undirected associative memory and the remaining layers form a belief net with directed, top‐down connections. We present efficient learning and inference procedures for this type of generative model and show that it allows more accurate and much faster retrieval than latent semantic analysis. By using our method as a filter for a much slower method called TF‐IDF we achieve higher accuracy than TF‐IDF alone and save several orders of magnitude in retrieval time. By using short binary codes as addresses, we can perform retrieval on very large document sets in a time that is independent of the size of the document set using only one word of memory to describe each document.",119회,"Discovering binary codes for documents by learning deep generative models
G Hinton, R Salakhutdinov - Topics in Cognitive Science, 2011
119회 인용 관련 학술자료 전체 14개의 버전",,,,
A survey on graph kernels,"Nils M Kriege, Fredrik D Johansson, Christopher Morris",2020/12,,5,1,1-42,SpringerOpen,"Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner’s guide to kernel-based graph classification.",117회,"A survey on graph kernels
NM Kriege, FD Johansson, C Morris - Applied Network Science, 2020
117회 인용 관련 학술자료 전체 9개의 버전",,,Applied Network Science,
Semi-supervised qa with generative domain-adaptive nets,"Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, William W Cohen",2017/2/7,arXiv preprint arXiv:1702.02206,,,,,"We study the problem of semi-supervised question answering----utilizing unlabeled text to boost the performance of question answering models. We propose a novel training framework, the Generative Domain-Adaptive Nets. In this framework, we train a generative model to generate questions based on the unlabeled text, and combine model-generated questions with human-generated questions for training question answering models. We develop novel domain adaptation algorithms, based on reinforcement learning, to alleviate the discrepancy between the model-generated data distribution and the human-generated data distribution. Experiments show that our proposed framework obtains substantial improvement from unlabeled text.",116회,"Semi-supervised qa with generative domain-adaptive nets
Z Yang, J Hu, R Salakhutdinov, WW Cohen - arXiv preprint arXiv:1702.02206, 2017
116회 인용 관련 학술자료 전체 9개의 버전",,,,
Adaptive overrelaxed bound optimization methods,"Ruslan Salakhutdinov, Sam Roweis",2003/8/21,,,,664-671,,"We study a class of overrelaxed bound optimization algorithms, and their relationship to standard bound optimizers, such as Expectation-Maximization, Iterative Scaling, CCCP and Non-Negative Matrix Factorization. We provide a theoretical analysis of the convergence properties of these optimizers and identify analytic conditions under which they are expected to outperform the standard versions. Based on this analysis, we propose a novel, simple adaptive overrelaxed scheme for practical optimization and report empirical results on several synthetic and real-world data sets showing that these new adaptive methods exhibit superior performance (in certain cases by several times speedup) compared to their traditional counterparts. Our extensions are simple to implement, apply to a wide variety of algorithms, almost always give a substantial speedup, and do not require any theoretical analysis of the underlying algorithm.",115회,"Adaptive overrelaxed bound optimization methods
R Salakhutdinov, ST Roweis - Proceedings of the 20th International Conference on …, 2003
115회 인용 관련 학술자료 전체 17개의 버전",International Conference on Machine Learning (ICML),,,
Multimodal transformer for unaligned multimodal language sequences,"Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, Ruslan Salakhutdinov",2019/7,Proceedings of the conference. Association for Computational Linguistics. Meeting,2019,,6558,NIH Public Access,"Human language is often multimodal, which comprehends a mixture of natural language, facial gestures, and acoustic behaviors. However, two major challenges in modeling such multimodal human language time-series data exist: 1) inherent data non-alignment due to variable sampling rates for the sequences from each modality; and 2) long-range dependencies between elements across modalities. In this paper, we introduce the Multimodal Transformer (MulT) to generically address the above issues in an end-to-end manner without explicitly aligning the data. At the heart of our model is the directional pairwise cross-modal attention, which attends to interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another. Comprehensive experiments on both aligned and non-aligned multimodal time-series show that our model outperforms state-of-the-art …",106회,"Multimodal transformer for unaligned multimodal language sequences
YHH Tsai, S Bai, PP Liang, JZ Kolter, LP Morency… - Proceedings of the conference. Association for …, 2019
106회 인용 관련 학술자료 전체 11개의 버전",,,,
Learning robust visual-semantic embeddings,"Yao-Hung Hubert Tsai, Liang-Kang Huang, Ruslan Salakhutdinov",2017,,,,3571-3580,,"Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (ie, auto-encoders) together with cross-domain learning criteria (ie, Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and few-shot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.",103회,"Learning robust visual-semantic embeddings
YH Hubert Tsai, LK Huang, R Salakhutdinov - Proceedings of the IEEE International Conference on …, 2017
103회 인용 관련 학술자료 전체 5개의 버전",Proceedings of the IEEE International Conference on Computer Vision,,,
Learning deep Boltzmann machines using adaptive MCMC,Ruslan Salakhutdinov,2010,,,,943-950,,"When modeling high-dimensional richly structured data, it is often the case that the distribution defined by the Deep Boltzmann Machine (DBM) has a rough energy landscape with many local minima separated by high energy barriers. The commonly used Gibbs sampler tends to get trapped in one local mode, which often results in unstable learning dynamics and leads to poor parameter estimates. In this paper, we concentrate on learning DBM’s using adaptive MCMC algorithms. We first show a close connection between Fast PCD and adaptive MCMC. We then develop a Coupled Adaptive Simulated Tempering algorithm that can be used to better explore a highly multimodal energy landscape. Finally, we demonstrate that the proposed algorithm considerably improves parameter estimates, particularly when learning large-scale DBM’s.",99회,"Learning deep Boltzmann machines using adaptive MCMC
R Salakhutdinov - Proceedings of the 27th International Conference on …, 2010
99회 인용 관련 학술자료 전체 12개의 버전",Proceedings of the 27th International Conference on Machine Learning (ICML-10),,,
Evaluating probabilities under high-dimensional latent variable models,"Iain Murray, Ruslan Salakhutdinov",2009,,,,,,"We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost.",97회,"Evaluating probabilities under high-dimensional latent variable models
I Murray, R Salakhutdinov - 2009
97회 인용 관련 학술자료 전체 18개의 버전",,,,
Exploiting compositionality to explore a large space of model structures,"Roger Grosse, Ruslan R Salakhutdinov, William T Freeman, Joshua B Tenenbaum",2012/10/16,,,,,,"The recent proliferation of richly structured probabilistic models raises the question of how to automatically determine an appropriate model for a dataset. We investigate this question for a space of matrix decomposition models which can express a variety of widely used models from unsupervised learning. To enable model selection, we organize these models into a context-free grammar which generates a wide variety of structures through the compositional application of a few simple rules. We use our grammar to generically and efficiently infer latent components and estimate predictive likelihood for nearly 2500 structures using a small toolbox of reusable algorithms. Using a greedy search over our grammar, we automatically choose the decomposition structure from raw data by evaluating only a small fraction of all models. The proposed method typically finds the correct structure for synthetic data and backs off gracefully to simpler models under heavy noise. It learns sensible structures for datasets as diverse as image patches, motion capture, 20 Questions, and US Senate votes, all using exactly the same code.",96회,"Exploiting compositionality to explore a large space of model structures
R Grosse, RR Salakhutdinov, WT Freeman… - arXiv preprint arXiv:1210.4856, 2012
96회 인용 관련 학술자료 전체 25개의 버전",Uncertainty in Artificial Intelligence (UAI),,,
On unifying deep generative models,"Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric P Xing",2017/6/2,arXiv preprint arXiv:1706.00550,,,,,"Deep generative models have achieved impressive success in recent years. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as emerging families for generative model learning, have largely been considered as two distinct paradigms and received extensive independent studies respectively. This paper aims to establish formal connections between GANs and VAEs through a new formulation of them. We interpret sample generation in GANs as performing posterior inference, and show that GANs and VAEs involve minimizing KL divergences of respective posterior and inference distributions with opposite directions, extending the two learning phases of classic wake-sleep algorithm, respectively. The unified view provides a powerful tool to analyze a diverse set of existing model variants, and enables to transfer techniques across research lines in a principled way. For example, we apply the importance weighting method in VAE literatures for improved GAN learning, and enhance VAEs with an adversarial mechanism that leverages generated samples. Experiments show generality and effectiveness of the transferred techniques.",95회,"On unifying deep generative models
Z Hu, Z Yang, R Salakhutdinov, EP Xing - arXiv preprint arXiv:1706.00550, 2017
95회 인용 관련 학술자료 전체 10개의 버전",,,,
Geometry of optimization and implicit regularization in deep learning,"Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, Nathan Srebro",2017/5/8,arXiv preprint arXiv:1705.03071,,,,,"We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.
Comments: This survey chapter was done as a part of Intel Collaborative Research institute for Computational Intelligence (ICRI-CI)"" Why & When Deep Learning works--looking inside Deep Learning"" compendium with the generous support of ICRI-CI. arXiv admin note: substantial text overlap with arXiv: 1506.02617",93회,"Geometry of optimization and implicit regularization in deep learning
B Neyshabur, R Tomioka, R Salakhutdinov, N Srebro - arXiv preprint arXiv:1705.03071, 2017
93회 인용 관련 학술자료 전체 3개의 버전",,,,
Multiple futures prediction,"Yichuan Charlie Tang, Ruslan Salakhutdinov",2019/11/4,arXiv preprint arXiv:1911.00997,,,,,"Temporal prediction is critical for making intelligent and robust decisions in complex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi-agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene. Our framework is data-driven and learns semantically meaningful latent variables to represent the multimodal future, without requiring explicit labels. Using a dynamic attention-based state encoder, we learn to encode the past as well as the future interactions among agents, efficiently scaling to any number of agents. Finally, our model can be used for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the'self'agent. We demonstrate our algorithms by predicting vehicle trajectories of both simulated and real data, demonstrating the state-of-the-art results on several vehicle trajectory datasets.",88회,"Multiple futures prediction
YC Tang, R Salakhutdinov - arXiv preprint arXiv:1911.00997, 2019
88회 인용 관련 학술자료 전체 4개의 버전",,,,
